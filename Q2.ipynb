{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif de la tâche:\n",
    "Le but de cet exercice est d'utiliser un réseau de neurones réccurrent pour prédire (ou compléter) le mot manquant dans un proverbe. Le RNN utilisé dans cette tâche est le LSTM (Long Short Term Memory). \n",
    "Pour ce faire, nous avons défini quelques fonctions utilitaires, le modèle LSTM qui sera entrainé puis évalué."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load dataset and some utils functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque proverbe, les plongements français de Spacy sont utilisés pour avoir son ***embedding*** correspondant que sera utilisé dans le LSTM.\n",
    "La fonction **get_vocabulary** est définie pour l'obtenetion d'un vocabulaire fermé i.e. un vocabulaire constitué unique des mots des documents de ***train*** et ***test***. Cette définition de vocabulaire fermé permet à notre modèle de connaître tous les mots dans le but d'éviter le problème des mots inconnus. La fonction ***load_dataset*** permet de constituer les corpus du train et du test. Ces derniers sont combinés pour former le vocabulaire fermé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_fr = spacy.load('fr_core_news_lg')\n",
    "spacy_embedding_dim = spacy_fr.meta['vectors']['width']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(corpus, spacy_analyzer, start_of_string=True, start_of_string_id=0, start_of_string_token='/<BOS>/', end_of_string=True, end_of_string_id=1, end_of_string_token='/<EOS>/'):\n",
    "    token_set = set()\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        doc = spacy_analyzer(sentence)\n",
    "        for token in doc:\n",
    "            token_set.add(token.text)\n",
    "    \n",
    "    vocabulary = list(token_set)\n",
    "\n",
    "    if start_of_string:\n",
    "        vocabulary.insert(start_of_string_id, start_of_string_token)\n",
    "\n",
    "    if end_of_string:\n",
    "        vocabulary.insert(end_of_string_id, end_of_string_token)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_dataset(path, format_type='txt'):    \n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        if format_type == 'json':\n",
    "            return json.load(f)\n",
    "\n",
    "        return [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_dataset = load_dataset('data/proverbes.txt')\n",
    "test_dataset = load_dataset('data/test_proverbes.txt', format_type='json')\n",
    "len(full_train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary is closed so we get the words of test dataset\n",
    "test_words = []\n",
    "for sentence_to_complete, propositions in test_dataset.items():\n",
    "    test_words.append(sentence_to_complete.replace(\" ***\", ''))\n",
    "    test_words.append(' '.join(propositions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocabulary(full_train_dataset + test_words, spacy_fr)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index: word for index, word in enumerate(vocab)}\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ProverbDataset*** est une classe qui hérite de ***Dataset*** de Pytorch. Cette classe est utilisée pour la préparation des doonées qui seront utilisées par l'entrainement, la validation et l'évaluation du modèle.\n",
    "Le principe ***ground truth*** est utilisé dans la fonction ***tokenize***. Celui constitue à définir, pour chaque mot, le mot suivant comme sa target. Cela permet d'entrainer le modèle avec la stratégie de type ***teacher forcing***. Pour chaque proverbe, des délimiteurs de phrase sont utilisés pour encadrer les tokens de celle-ci. Les délimiteurs ***<BOS>*** et ***<EOS>*** sont utilisés pour matérialiser le début et la fin de la phrase respectivement. La fonction ***fill_dataset_with_targets*** donne toutes les données et targets d'un dataset donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import LongTensor, FloatTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class ProverbDataset(Dataset):\n",
    "    def __init__(self, dataset, word_to_index, spacy_analyzer):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = spacy_analyzer\n",
    "        self.word_to_index = word_to_index\n",
    "        # Construct dataset such as the next word is a target of the previous\n",
    "        self.dataset_with_targets = self.fill_dataset_with_targets() # [w(0), w(1), ...w(n-1)],[w(1), w(2), ...w(n)] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_with_targets[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return LongTensor([self.dataset_with_targets[0][index]]), FloatTensor([self.dataset_with_targets[1][index]]).squeeze(0)\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        tokens = [word.text for word in self.tokenizer(sentence)]\n",
    "        tokens.insert(0, '/<BOS>/')\n",
    "        tokens.append('/<EOS>/')\n",
    "\n",
    "        data = []\n",
    "        targets = []\n",
    "        for index in range(len(tokens)-1):\n",
    "            data.append(self.word_to_index.get(tokens[index], 1))\n",
    "            targets.append(self.word_to_index.get(tokens[index+1], 1))\n",
    "\n",
    "        return data, targets\n",
    "\n",
    "    def fill_dataset_with_targets(self):\n",
    "        full_data = []\n",
    "        full_targets = []\n",
    "        for sentence in self.dataset:\n",
    "            data, targets = self.tokenize(sentence)\n",
    "            full_data.extend(data)\n",
    "            full_targets.extend(targets)\n",
    "\n",
    "        assert len(full_data) == len(full_targets)\n",
    "        return full_data, full_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Division du jeu de données d'entrainement en des sous-ensembles de train et de validation**\n",
    "Cette approche permet d'entrainer puis de valider notre modèle avant de l'évaluer avec les données du test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.1\n",
    "valid_size = int(len(full_train_dataset) * valid_ratio)\n",
    "train_size = len(full_train_dataset) - valid_size\n",
    "\n",
    "X_train = full_train_dataset[:train_size]\n",
    "X_valid = full_train_dataset[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Définition des différentes variables et constantes qui seront utilisées pour la définition du modèle LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get spacy embeddings of all the words in vocabulary\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = spacy_embedding_dim\n",
    "embedding_layer = np.zeros((vocab_size, embedding_size), dtype=np.float32)\n",
    "for index, word in index_to_word.items():\n",
    "    embedding_layer[index, :] = spacy_fr(word).vector\n",
    "\n",
    "embedding_layer = torch.from_numpy(embedding_layer)\n",
    "embedding_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle LSTM est défini par les vecteurs de plongements définis précédemment, ***embedding_layer***. Ceux-ci sont utilisés pour définir la taille des vecteurs de plongements et la couche de plongement du LSTM. Ensuite, les dimensions des vecteurs des couches cachée et de sortie sont initialisées, ***hidden_dim et output_dim***. Pour la récurrence du modèle, un autre LSTM est défini dans le modèle. Pour connecter les couches cachée et de sortie, la fonction linéaire est utilisée.\n",
    "Pour l'entrainement, ***forward***, l'input est est passé au travers de la couche de plongements, ***embedding_layer*** pour définir son vecteur de plongement. Le vecteur obtenu est passé à la couche ***lstm_layer*** pour donner la prédiction et le couple hidden et context qui sera utilisé dans la constitution des prochains inputs. Le vecteur de prédiction résultant est compressé puis passe au travers de la couche linéaire, ***fc_layer*** pour déterminer la probabilité de sortie de l'input.\n",
    "\n",
    "Pour entrainer notre modèle, on constitue des séquences de deux mots [$w_{i-1}$ $wi$] où $wi$ est considéré comme la target de $w{i-1}$. \n",
    "Pour chaque phrase dans le dataset, on découpe la phrase en bigrammes et on renvoie pour l'index correspondant les tensors correspondants qui seront ensuite chargés en minibatch grâce au Dataloader. \n",
    "\n",
    "Pour chaque minibatch tiré de notre loader, on le passe dans notre modèle lstm constitué d'une couche d'embeddings préentrainée de spacy (len(vocabulary)xspacy_embedding_dim), une couche lstm (spacy_embedding_dim x hidden_dim) et d'une couche pleinement connectée (hidden_dim, output_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ModelLanguageLSTM(nn.Module):\n",
    "    def __init__(self, embeddings, hidden_dim, output_dim):\n",
    "        super(ModelLanguageLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_layer = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.embedding_size = embeddings.size()[1]\n",
    "        self.lstm_layer = nn.LSTM(self.embedding_size, hidden_dim, 1, batch_first=True, bidirectional=False)\n",
    "        self.fc_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x, (h, ctx) = self.lstm_layer(x)\n",
    "        x = h.squeeze()\n",
    "        x = self.fc_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with poutyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 4\n",
    "output_dim = 1\n",
    "\n",
    "lstm = ModelLanguageLSTM(embedding_layer, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProverbDataset(X_train, word_to_index, spacy_fr)\n",
    "valid_dataset = ProverbDataset(X_valid, word_to_index, spacy_fr)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('results/Q2')\n",
    "except:\n",
    "    print('Ignored!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y_target):\n",
    "    y_pred = torch.nn.functional.softmax(y_pred.squeeze(1))\n",
    "    return nn.functional.cross_entropy(y_pred, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/25 \u001b[35mStep: \u001b[36m  59/2079 \u001b[35m  2.84% |\u001b[35m▌                   \u001b[35m|\u001b[35mETA: \u001b[32m5.47s \u001b[35mloss:\u001b[94m 89788.851562\u001b[35m acc:\u001b[94m 0.000000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wansijou\\AppData\\Local\\Temp\\ipykernel_25728\\921777836.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = torch.nn.functional.softmax(y_pred.squeeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m 1/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m4.39s \u001b[35mloss:\u001b[94m 90594.325867\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88277.338753\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Epoch 1: val_loss improved from inf to 88277.33875, saving file to results/Q2/checkpoint_epoch_1.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 2/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.67s \u001b[35mloss:\u001b[94m 90592.600836\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88208.808919\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Epoch 2: val_loss improved from 88277.33875 to 88208.80892, saving file to results/Q2/checkpoint_epoch_2.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 3/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.75s \u001b[35mloss:\u001b[94m 90603.294334\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88289.188115\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 4/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.92s \u001b[35mloss:\u001b[94m 90593.612790\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88232.159515\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 5/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.71s \u001b[35mloss:\u001b[94m 90611.902362\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88306.494701\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 6/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.80s \u001b[35mloss:\u001b[94m 90603.887892\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88260.198904\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 7/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.74s \u001b[35mloss:\u001b[94m 90606.764779\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88171.553256\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Epoch 7: val_loss improved from 88208.80892 to 88171.55326, saving file to results/Q2/checkpoint_epoch_7.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m 8/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.79s \u001b[35mloss:\u001b[94m 90594.148909\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88210.337634\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m 9/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.69s \u001b[35mloss:\u001b[94m 90606.178891\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88190.557463\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m10/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.73s \u001b[35mloss:\u001b[94m 90590.964511\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88289.830567\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m11/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.94s \u001b[35mloss:\u001b[94m 90612.061606\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88228.772197\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m12/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m4.37s \u001b[35mloss:\u001b[94m 90616.885833\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88171.380671\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Epoch 12: val_loss improved from 88171.55326 to 88171.38067, saving file to results/Q2/checkpoint_epoch_12.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m13/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.73s \u001b[35mloss:\u001b[94m 90613.569049\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88257.665652\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m14/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.68s \u001b[35mloss:\u001b[94m 90605.387470\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88182.530650\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m15/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.69s \u001b[35mloss:\u001b[94m 90618.671735\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88240.122615\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m16/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.68s \u001b[35mloss:\u001b[94m 90616.364669\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88169.518815\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Epoch 16: val_loss improved from 88171.38067 to 88169.51882, saving file to results/Q2/checkpoint_epoch_16.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m17/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.70s \u001b[35mloss:\u001b[94m 90641.671460\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88195.875866\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m18/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.67s \u001b[35mloss:\u001b[94m 90604.862559\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88224.959348\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m19/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.66s \u001b[35mloss:\u001b[94m 90604.468635\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88298.144464\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m20/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.68s \u001b[35mloss:\u001b[94m 90677.484590\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88171.771151\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m21/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.72s \u001b[35mloss:\u001b[94m 90672.108525\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88287.439662\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m22/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.78s \u001b[35mloss:\u001b[94m 90621.786634\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88285.404973\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m23/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m4.14s \u001b[35mloss:\u001b[94m 90558.142078\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88284.736566\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "\u001b[35mEpoch: \u001b[36m24/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.76s \u001b[35mloss:\u001b[94m 90568.316324\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88110.864778\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Epoch 24: val_loss improved from 88169.51882 to 88110.86478, saving file to results/Q2/checkpoint_epoch_24.ckpt\n",
      "\u001b[35mEpoch: \u001b[36m25/25 \u001b[35mTrain steps: \u001b[36m2079 \u001b[35mVal steps: \u001b[36m218 \u001b[32m3.70s \u001b[35mloss:\u001b[94m 90605.336598\u001b[35m acc:\u001b[94m 0.000000\u001b[35m val_loss:\u001b[94m 88166.414319\u001b[35m val_acc:\u001b[94m 0.000000\u001b[0m\n",
      "Restoring data from results/Q2/checkpoint_epoch_24.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'epoch': 1,\n",
       "  'time': 4.394115600000077,\n",
       "  'loss': 90594.32586692131,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88277.3387531241,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 2,\n",
       "  'time': 3.6748869999998988,\n",
       "  'loss': 90592.60083612052,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88208.80891852165,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 3,\n",
       "  'time': 3.7532951999999113,\n",
       "  'loss': 90603.29433428704,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88289.1881147062,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 4,\n",
       "  'time': 3.9187580999998772,\n",
       "  'loss': 90593.61279048718,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88232.15951457314,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 5,\n",
       "  'time': 3.714496299999837,\n",
       "  'loss': 90611.90236204605,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88306.4947013899,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 6,\n",
       "  'time': 3.7970066999998835,\n",
       "  'loss': 90603.88789181145,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88260.19890353046,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 7,\n",
       "  'time': 3.7358277999999245,\n",
       "  'loss': 90606.76477856556,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88171.55325580768,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 8,\n",
       "  'time': 3.7887342000001354,\n",
       "  'loss': 90594.14890914448,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88210.33763440377,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 9,\n",
       "  'time': 3.6894241999998485,\n",
       "  'loss': 90606.1788910493,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88190.55746323001,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 10,\n",
       "  'time': 3.728749599999901,\n",
       "  'loss': 90590.96451124946,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88289.83056696814,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 11,\n",
       "  'time': 3.941017800000054,\n",
       "  'loss': 90612.06160625428,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88228.77219684982,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 12,\n",
       "  'time': 4.372173200000134,\n",
       "  'loss': 90616.88583266923,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88171.3806705804,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 13,\n",
       "  'time': 3.7340454000000136,\n",
       "  'loss': 90613.56904901555,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88257.66565152115,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 14,\n",
       "  'time': 3.678924600000073,\n",
       "  'loss': 90605.38746982768,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88182.53064990291,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 15,\n",
       "  'time': 3.685847899999999,\n",
       "  'loss': 90618.67173497504,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88240.1226153445,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 16,\n",
       "  'time': 3.679020600000058,\n",
       "  'loss': 90616.36466857784,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88169.51881540385,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 17,\n",
       "  'time': 3.6996216999998524,\n",
       "  'loss': 90641.67146047561,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88195.87586643232,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 18,\n",
       "  'time': 3.66869870000005,\n",
       "  'loss': 90604.86255927567,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88224.95934847885,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 19,\n",
       "  'time': 3.659868299999971,\n",
       "  'loss': 90604.4686347026,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88298.14446382337,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 20,\n",
       "  'time': 3.679641599999968,\n",
       "  'loss': 90677.48458991248,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88171.77115061313,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 21,\n",
       "  'time': 3.722060699999929,\n",
       "  'loss': 90672.10852515891,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88287.43966214759,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 22,\n",
       "  'time': 3.7810755999998946,\n",
       "  'loss': 90621.78663379526,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88285.40497282706,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 23,\n",
       "  'time': 4.144369099999949,\n",
       "  'loss': 90558.14207822885,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88284.7365663658,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 24,\n",
       "  'time': 3.757738200000176,\n",
       "  'loss': 90568.31632384437,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88110.86477812141,\n",
       "  'val_acc': 0.0},\n",
       " {'epoch': 25,\n",
       "  'time': 3.6993761000001086,\n",
       "  'loss': 90605.33659759669,\n",
       "  'acc': 0.0,\n",
       "  'val_loss': 88166.41431872123,\n",
       "  'val_acc': 0.0}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from poutyne import set_seeds\n",
    "from poutyne.framework import Experiment\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "experiment = Experiment(\"results/Q2/\", lstm, loss_function=loss_fn, batch_metrics=[\"acc\"], optimizer=\"SGD\")\n",
    "experiment.train(train_dataloader, valid_dataloader, epochs=25, disable_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test and evaluate lstm model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle entrainé est utilisé pour la prédiction du mot manquant. La démarche adoptée est la suivante:\n",
    "- Pour chaque proverbe de test, le mot manquant est remplacé par une des propositions\n",
    "- Le proverbe \"complet\" obtenu est tokenisé\n",
    "- Les tokens sont ensuite convertis en une liste de leurs index respectifs. Celle est transformée en tensor avant d'être utilisé par le modèle pour donner la probabilité. \n",
    "- La proposition du proverbe avec la plus grande probabilité est sélectionnée comme le mot manqaunt idéal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sentence_to_complete, propositions, target_token=\"***\"):\n",
    "    probs = []\n",
    "    possible_proverbs = [sentence_to_complete.replace(target_token, w) for w in propositions]\n",
    "    for p in possible_proverbs:\n",
    "        tokens = [word.text for word in spacy_fr(p)]\n",
    "        tokens.insert(0, '/<BOS>/')\n",
    "        tokens.append('/<EOS>/')\n",
    "\n",
    "        data = []\n",
    "        for idx in range(len(tokens)-1):\n",
    "            data.append(word_to_index[tokens[idx]])\n",
    "\n",
    "        out = lstm(LongTensor(data))\n",
    "\n",
    "    to_predict = np.argmax(np.average(out.detach().numpy()))\n",
    "    return propositions[to_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a beau mentir qui *** de loin ----> vient\n",
      "a beau *** qui vient de loin ----> mentir\n",
      "l’occasion fait le *** ----> larron\n",
      "aide-toi, le ciel t’*** ----> aidera\n",
      "année de gelée, *** de blé ----> année\n",
      "après la pluie, le *** temps ----> beau\n",
      "aux échecs, les *** sont les plus près des rois ----> fous\n",
      "ce que *** veut, dieu le veut ----> femme\n",
      "bien mal acquis ne *** jamais ----> profite\n",
      "bon ouvrier ne querelle pas ses *** ----> outils\n",
      "ce n’est pas tous les jours *** ----> fête\n",
      "pour le fou, c’est tous les jours *** ----> fête\n",
      "dire et faire, *** deux ----> sont\n",
      "mieux vaut *** que jamais ----> tard\n",
      "d’un sac *** ne peut tirer deux moutures ----> on\n",
      "à qui dieu aide, *** ne peut nuire ----> nul\n",
      "il n’y a *** de rose de cent jours ----> point\n",
      "il faut le *** pour le croire ----> voir\n",
      "on ne *** pas le poisson qui est encore dans la mer ----> vend\n",
      "la langue d’un *** vaut mieux que celle d’un menteur ----> muet\n",
      "*** femme fait le bon homme ----> bonne\n",
      "bonne *** fait le bon homme ----> femme\n",
      "bonne femme *** le bon homme ----> fait\n",
      "bonne femme fait *** bon homme ----> le\n",
      "bonne femme fait le *** homme ----> bon\n",
      "bonne femme fait le bon *** ----> homme\n",
      "*** que femme veut, dieu le veut ----> ce\n",
      "ce *** femme veut, dieu le veut ----> que\n",
      "ce que femme ***, dieu le veut ----> veut\n",
      "ce que femme veut, dieu le *** ----> veut\n",
      "*** parleur, grand menteur ----> grand\n",
      "grand ***, grand menteur ----> parleur\n",
      "grand parleur, *** menteur ----> grand\n",
      "grand parleur, grand *** ----> menteur\n",
      "*** poisson pourrit par la tête ----> le\n",
      "le poisson *** par la tête ----> pourrit\n",
      "le poisson pourrit par la *** ----> tête\n",
      "*** vaut prévenir que guérir ----> mieux\n",
      "mieux *** prévenir que guérir ----> vaut\n",
      "mieux vaut *** que guérir ----> prévenir\n",
      "mieux vaut prévenir *** guérir ----> que\n",
      "mieux vaut prévenir que *** ----> guérir\n",
      "*** la poire est mûre, elle tombe ----> quand\n",
      "quand la *** est mûre, elle tombe ----> poire\n",
      "quand la poire *** mûre, elle tombe ----> est\n",
      "quand la poire est mûre, elle *** ----> tombe\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "# Get all predictions of our test dataset\n",
    "for sentence_to_complete, propositions in test_dataset.items():\n",
    "    pred = get_prediction(sentence_to_complete, propositions)\n",
    "    predictions.append(pred)\n",
    "    print(f\"{sentence_to_complete} ----> {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've created a file with the solution of the sentence in test dataset\n",
    "solutions = load_dataset(\"data/solutions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_metric(y_true, y_pred):\n",
    "    true_pred = 0\n",
    "\n",
    "    for idx in range(len(y_pred)):\n",
    "        if y_true[idx] == y_pred[idx]:\n",
    "            true_pred += 1\n",
    "    \n",
    "    return true_pred / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9347826086956522"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_metric(solutions, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusion, force est de constater que le LSTM créé est très instable. Après plusieurs tests, nous avons réussi à avoir une précision de 93%. Cependant, cette très bonne valeur de la précision du modèle nous laisse perplexe sur la robustesse du fait l'instabilité du modèle. En effet, les différentes précisions obtenues au cours de l'expérimentation varient entre 20 et 93%. Cette instabilité pourrait être expliquée par l'approche ground truth. Avec cette stratégie, un mot peut se retrouver à avoir plusieurs targets possibles. De ce fait, le modèle fera une sélection aléatoire. En perspective, il serait intéressant de faire un LSTM bidirectionnel pour avoir beaucoup plus de contexte afin d'aider le modèle à prendre la meilleure décision. \n",
    "\n",
    "Par comparaison au modèle du TP1 qui utilisait l'algorithme Laplace, le LSTM semble être beaucoup moins robuste. Les résultats du LSTM ressemblent à ceux du modèle Laplace en unigramme. Cela nous pousse à nous poser la question sur notre algorithme de ground truth. Il serait intéressant alors d'utiliser un algorithme dans lequel les données et targets sont constituées graduellement "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "49699c52b263e701306c3825e4e0955b10b1ee5105ccd03e8f8674a61ce380a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
